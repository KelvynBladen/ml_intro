{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset your data the same 3 digit classes you used in the Random Forest problem from the last assignment.\n",
    "\n",
    "# Again, You can totally copy any code over from the fmnist_examples notebook and modify it.\n",
    "# You can use chatGPT, copilot, google, or other AI or online resources.\n",
    "# Use each other, the goal is to complete the objectives and maybe learn something new, not to struggle to make up code on your own.\n",
    "# We have prior notebooks, chatGPT, the internet, and each other for the rapids projects, so use them here if they will help!\n",
    "\n",
    "# Extract the 3 classes from the training data\n",
    "\n",
    "train_filter = (y_train == 5) | (y_train == 6)\n",
    "x_train_nn = x_train[train_filter]\n",
    "y_train_nn = y_train[train_filter]\n",
    "\n",
    "# Extract the 3 classes from the test data\n",
    "\n",
    "test_filter = (y_test == 5) | (y_test == 6)\n",
    "x_test_nn = x_test[test_filter]\n",
    "y_test_nn = y_test[test_filter]\n",
    "\n",
    "print(y_train_nn)\n",
    "y_train_nn_bin = (y_train_nn == 5).astype(int)\n",
    "y_test_nn_bin = (y_test_nn == 5).astype(int)\n",
    "print(y_train_nn_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot 6 to 10 images from your data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(x_train_nn[i], cmap='gray')\n",
    "    plt.title(f'Label: {y_train_nn_bin[i]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Pytorth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model. \n",
    "# You are welcome to use the model from the fmnist_examples notebook, or you can try to create a better one.\n",
    "# Either way, I recommend starting with the model from the fmnist_examples notebook and modifying it as you like.\n",
    "\n",
    "# Important: Remember that our output has 3 classes, not 2 like in the Fashion MNIST dataset,\n",
    "# So you will have to change at least one thing in the model to accommodate this.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data to PyTorch tensors and create DataLoader objects for training and testing\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_nn.reshape(-1, 1, 28, 28), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_nn_bin, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test_nn.reshape(-1, 1, 28, 28), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_nn_bin, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "t_model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(t_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# More epochs takes more time, but also helps your model to be more accurate.\n",
    "# Report some notion of accuracy, loss or both as it builds to see how well your model is doing.\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    t_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = t_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    t_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in test_loader:\n",
    "            val_outputs = t_model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    # Print metrics rounded to 4 decimals\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Validation Loss: {avg_val_loss:.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "\n",
    "# How does the test accuracy compare to that of the Random Forest model previously used?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot a confusion matrix.\n",
    "\n",
    "'Your code here'\n",
    "\n",
    "# Where are misclassifications occurring?\n",
    "# How does your test accuracy compare to that of the Random Forest model previously made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the misclassifications.\n",
    "\n",
    "'Your code here'\n",
    "\n",
    "# Can you see why the model made the misclassifications it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement some kind of Appropriate Data Transformation(s) for this data set.\n",
    "# You can/should copy the train_mod function from the fmnist_examples_ml2 notebook, but you will have to \n",
    "# think about whether all of the transformations are appropriate for this data set and maybe adjust them.\n",
    "\n",
    "# There is 1 specific modification you must make to the train_mod function! Other modifications are great, but optional.\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation transformations\n",
    "train_mod = transforms.Compose([\n",
    "    \"Your code here\",\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))    # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "test_mod = transforms.Compose([\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))       # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the MNIST train dataset\n",
    "train_dataset = MNIST(root=\"./data\", train=True, transform=train_mod, download=True)\n",
    "\n",
    "# Filter the dataset to only include the 3 classes of interest\n",
    "# Convert to labels to 0, 1, 2\n",
    "# Convert the dataset to a DataLoader object\n",
    "\n",
    "\"Your code here\"\n",
    "\n",
    "# Repeat for the loading, filtering, and conversions on the test dataset\n",
    "test_dataset = MNIST(root=\"./data\", train=False, transform=test_mod, download=True)\n",
    "\n",
    "'Your code here'\n",
    "\n",
    "# Plot some of the images in your train_loader. Check that your transformations provide\n",
    "# a reasonable augmentation of the data. If they don't, adjust them in the train_mod function.\n",
    "\n",
    "'Your code here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your CNN using the data loaders containing different transformations.\n",
    "# This very well may not be better than the model trained on the original data, \n",
    "# since we don't expect our test data to be transformed. But try it out.\n",
    "\n",
    "# Provide initializations for a new model and train the model.\n",
    "# Report some notion of accuracy, loss or both as it builds to see how well your model is doing.\n",
    "\n",
    "'Your code here'\n",
    "\n",
    "# How does the test accuracy compare to other models you've built?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
